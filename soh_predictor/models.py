"""
ëª¨ë¸ ëª¨ë“ˆ (ì™„ì„±ë³¸)
==================

SOH ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ, í‰ê°€, ì˜ˆì¸¡
"""

from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
)
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, cross_val_score, GroupKFold, KFold
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression

from .config import Config
from .utils import logger

# Optional imports
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False


class ModelTrainer:
    """ëª¨ë¸ í•™ìŠµê¸°"""
    
    def __init__(self, config: Config):
        self.config = config
        self.model_config = config.model
        self.models: Dict[str, Any] = {}
        self.scaler = RobustScaler()
        self.feature_selector: Optional[SelectKBest] = None
        self.feature_names: List[str] = []
        self.ensemble_weights: Dict[str, float] = {}
        self._X_scaled: Optional[np.ndarray] = None
    
    def train(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        groups: Optional[pd.Series] = None,
        use_cv: bool = True,
        grid_search: bool = True,
        random_state: Optional[int] = None,
    ) -> Dict[str, Dict[str, float]]:
        """ëª¨ë¸ í•™ìŠµ"""
        random_state = random_state or self.model_config.random_state
        logger.info(f"ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (random_state={random_state})")
        
        # ì „ì²˜ë¦¬
        self._X_scaled = self._preprocess_features(X, y)
        
        # êµì°¨ ê²€ì¦ ì„¤ì •
        cv_splits = self._setup_cross_validation(self._X_scaled, y, groups, random_state)
        
        # ëª¨ë¸ í•™ìŠµ
        results = {}
        results.update(self._train_random_forest(self._X_scaled, y, cv_splits, random_state, grid_search))
        results.update(self._train_gradient_boosting(self._X_scaled, y, cv_splits, random_state, grid_search))
        
        if XGBOOST_AVAILABLE and grid_search:
            results.update(self._train_xgboost(self._X_scaled, y, cv_splits, random_state))
        
        if LIGHTGBM_AVAILABLE and grid_search:
            results.update(self._train_lightgbm(self._X_scaled, y, cv_splits, random_state))
        
        if grid_search and len(self.models) >= 2:
            results.update(self._train_stacking(self._X_scaled, y, cv_splits))
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°
        self._compute_ensemble_weights(results)
        
        self._log_results(results)
        return results
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """SOH ì˜ˆì¸¡"""
        X_processed = self._prepare_for_prediction(X)
        
        predictions = {}
        for model_name, model in self.models.items():
            predictions[model_name] = model.predict(X_processed)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_pred = np.zeros(len(X))
        for model_name, pred in predictions.items():
            weight = self.ensemble_weights.get(model_name, 1.0 / len(self.models))
            ensemble_pred += pred * weight
        
        return ensemble_pred
    
    def save(self, filepath: Path) -> None:
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'models': self.models,
            'scaler': self.scaler,
            'feature_selector': self.feature_selector,
            'feature_names': self.feature_names,
            'ensemble_weights': self.ensemble_weights,
            'config': self.config,
        }
        joblib.dump(model_data, filepath)
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load(self, filepath: Path) -> None:
        """ëª¨ë¸ ë¡œë“œ"""
        model_data = joblib.load(filepath)
        self.models = model_data['models']
        self.scaler = model_data['scaler']
        self.feature_selector = model_data.get('feature_selector')
        self.feature_names = model_data['feature_names']
        self.ensemble_weights = model_data['ensemble_weights']
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
    
    def _preprocess_features(self, X: pd.DataFrame, y: pd.Series) -> np.ndarray:
        """í”¼ì²˜ ì „ì²˜ë¦¬"""
        logger.info("ğŸ“Š í”¼ì²˜ ì„ íƒ ì¤‘...")
        
        X_clean = X.fillna(0).replace([np.inf, -np.inf], 0)
        
        n_features = min(self.model_config.n_features_max, len(X_clean.columns))
        self.feature_selector = SelectKBest(score_func=f_regression, k=n_features)
        X_selected = self.feature_selector.fit_transform(X_clean, y)
        
        self.feature_names = X_clean.columns[self.feature_selector.get_support()].tolist()
        logger.info(f"  {len(X_clean.columns)}ê°œ â†’ {len(self.feature_names)}ê°œ í”¼ì²˜")
        
        return self.scaler.fit_transform(X_selected)
    
    def _prepare_for_prediction(self, X: pd.DataFrame) -> np.ndarray:
        """ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        X_clean = X.fillna(0).replace([np.inf, -np.inf], 0)
        
        # ëˆ„ë½ëœ í”¼ì²˜ ì¶”ê°€
        for col in self.feature_names:
            if col not in X_clean.columns:
                X_clean[col] = 0
        
        # í”¼ì²˜ ì„ íƒ ì ìš©
        if self.feature_selector is not None:
            X_selected = self.feature_selector.transform(X_clean)
        else:
            X_selected = X_clean[self.feature_names].values
        
        return self.scaler.transform(X_selected)
    
    def _setup_cross_validation(
        self,
        X: np.ndarray,
        y: pd.Series,
        groups: Optional[pd.Series],
        random_state: int,
    ) -> List[Tuple[np.ndarray, np.ndarray]]:
        """êµì°¨ ê²€ì¦ ì„¤ì •"""
        n_splits = self.model_config.cv_folds
        
        if groups is not None:
            n_groups = len(np.unique(groups))
            n_splits = min(n_splits, n_groups)
            
            if n_splits >= 2:
                cv = GroupKFold(n_splits=n_splits)
                logger.info(f"ğŸ“Š GroupKFold ({n_splits}-fold, {n_groups}ê°œ ê·¸ë£¹)")
                return list(cv.split(X, y, groups))
        
        cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
        logger.info(f"ğŸ“Š KFold ({n_splits}-fold)")
        return list(cv.split(X, y))
    
    def _train_random_forest(
        self,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
        random_state: int,
        grid_search: bool,
    ) -> Dict[str, Dict]:
        """Random Forest í•™ìŠµ"""
        logger.info("ğŸŒ² Random Forest í•™ìŠµ ì¤‘...")
        
        params = self.model_config.rf_params if grid_search else {
            k: [v[0]] for k, v in self.model_config.rf_params.items()
        }
        
        grid = GridSearchCV(
            RandomForestRegressor(random_state=random_state, n_jobs=-1),
            params, cv=cv_splits, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=0
        )
        grid.fit(X, y)
        self.models['RandomForest'] = grid.best_estimator_
        
        logger.info(f"  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid.best_params_}")
        return {'RandomForest': self._evaluate_model(grid.best_estimator_, X, y, cv_splits)}
    
    def _train_gradient_boosting(
        self,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
        random_state: int,
        grid_search: bool,
    ) -> Dict[str, Dict]:
        """Gradient Boosting í•™ìŠµ"""
        logger.info("ğŸ“ˆ Gradient Boosting í•™ìŠµ ì¤‘...")
        
        params = self.model_config.gb_params if grid_search else {
            k: [v[0]] for k, v in self.model_config.gb_params.items()
        }
        
        grid = GridSearchCV(
            GradientBoostingRegressor(random_state=random_state),
            params, cv=cv_splits, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=0
        )
        grid.fit(X, y)
        self.models['GradientBoosting'] = grid.best_estimator_
        
        logger.info(f"  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid.best_params_}")
        return {'GradientBoosting': self._evaluate_model(grid.best_estimator_, X, y, cv_splits)}
    
    def _train_xgboost(
        self,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
        random_state: int,
    ) -> Dict[str, Dict]:
        """XGBoost í•™ìŠµ"""
        logger.info("âš¡ XGBoost í•™ìŠµ ì¤‘...")
        
        try:
            grid = GridSearchCV(
                xgb.XGBRegressor(random_state=random_state, n_jobs=-1),
                self.model_config.xgb_params,
                cv=cv_splits, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=0
            )
            grid.fit(X, y)
            self.models['XGBoost'] = grid.best_estimator_
            
            logger.info(f"  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid.best_params_}")
            return {'XGBoost': self._evaluate_model(grid.best_estimator_, X, y, cv_splits)}
        except Exception as e:
            logger.warning(f"  âš ï¸ XGBoost í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {}
    
    def _train_lightgbm(
        self,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
        random_state: int,
    ) -> Dict[str, Dict]:
        """LightGBM í•™ìŠµ"""
        logger.info("ğŸ’¡ LightGBM í•™ìŠµ ì¤‘...")
        
        try:
            grid = GridSearchCV(
                lgb.LGBMRegressor(random_state=random_state, n_jobs=-1, verbose=-1),
                self.model_config.lgb_params,
                cv=cv_splits, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=0
            )
            grid.fit(X, y)
            self.models['LightGBM'] = grid.best_estimator_
            
            logger.info(f"  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid.best_params_}")
            return {'LightGBM': self._evaluate_model(grid.best_estimator_, X, y, cv_splits)}
        except Exception as e:
            logger.warning(f"  âš ï¸ LightGBM í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {}
    
    def _train_stacking(
        self,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
    ) -> Dict[str, Dict]:
        """Stacking ì•™ìƒë¸” í•™ìŠµ"""
        logger.info("ğŸ¯ Stacking ì•™ìƒë¸” êµ¬ì„± ì¤‘...")
        
        estimators = [(name.lower(), model) for name, model in self.models.items()]
        
        try:
            stacking = StackingRegressor(
                estimators=estimators,
                final_estimator=Ridge(alpha=1.0),
                cv=cv_splits, n_jobs=-1
            )
            stacking.fit(X, y)
            self.models['Stacking'] = stacking
            
            logger.info("  âœ… Stacking ì•™ìƒë¸” ì™„ë£Œ")
            return {'Stacking': self._evaluate_model(stacking, X, y, cv_splits)}
        except Exception as e:
            logger.warning(f"  âš ï¸ Stacking ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
            return {}
    
    def _evaluate_model(
        self,
        model: Any,
        X: np.ndarray,
        y: pd.Series,
        cv_splits: List,
    ) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        mae_scores = cross_val_score(model, X, y, cv=cv_splits, scoring='neg_mean_absolute_error')
        r2_scores = cross_val_score(model, X, y, cv=cv_splits, scoring='r2')
        
        result = {
            'mae_mean': -mae_scores.mean(),
            'mae_std': mae_scores.std(),
            'r2_mean': r2_scores.mean(),
            'r2_std': r2_scores.std(),
        }
        
        logger.info(f"  CV MAE: {result['mae_mean']:.4f}% (Â±{result['mae_std']:.4f}%)")
        logger.info(f"  CV RÂ²: {result['r2_mean']:.4f} (Â±{result['r2_std']:.4f})")
        
        return result
    
    def _compute_ensemble_weights(self, results: Dict[str, Dict]) -> None:
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° (MAE ê¸°ë°˜)"""
        if not results:
            return
        
        total_mae = sum(r['mae_mean'] for r in results.values())
        
        for name, res in results.items():
            weight = (1 - res['mae_mean'] / total_mae) / len(results) if total_mae > 0 else 1 / len(results)
            self.ensemble_weights[name] = max(weight, 0)
        
        # ì •ê·œí™”
        total = sum(self.ensemble_weights.values())
        if total > 0:
            self.ensemble_weights = {k: v / total for k, v in self.ensemble_weights.items()}
        
        logger.info("\nâš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜:")
        for name, weight in self.ensemble_weights.items():
            logger.info(f"  {name}: {weight:.3f}")
    
    def _log_results(self, results: Dict[str, Dict]) -> None:
        """ê²°ê³¼ ë¡œê¹…"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:")
        logger.info("-" * 60)
        
        for name, res in results.items():
            logger.info(
                f"  {name:15s}: MAE {res['mae_mean']:.4f}% (Â±{res['mae_std']:.4f}%), "
                f"RÂ² {res['r2_mean']:.4f} (Â±{res['r2_std']:.4f})"
            )
        
        if results:
            best = min(results.keys(), key=lambda k: results[k]['mae_mean'])
            logger.info(f"\nğŸ† ìµœê³  ëª¨ë¸: {best}")


class SOHPredictor:
    """SOH ì˜ˆì¸¡ê¸° (í•™ìŠµëœ ModelTrainer ë˜í¼)"""
    
    def __init__(self, trainer: ModelTrainer):
        """
        Args:
            trainer: í•™ìŠµ ì™„ë£Œëœ ModelTrainer ì¸ìŠ¤í„´ìŠ¤
        """
        self.trainer = trainer
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        SOH ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ í”¼ì²˜ DataFrame
        
        Returns:
            SOH ì˜ˆì¸¡ê°’ ë°°ì—´ (%)
        """
        return self.trainer.predict(X)
    
    def predict_with_uncertainty(
        self, X: pd.DataFrame
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨í•œ SOH ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ í”¼ì²˜ DataFrame
        
        Returns:
            (ì˜ˆì¸¡ê°’ ë°°ì—´, í‘œì¤€í¸ì°¨ ë°°ì—´) íŠœí”Œ
        """
        X_processed = self.trainer._prepare_for_prediction(X)
        
        predictions = []
        for model in self.trainer.models.values():
            predictions.append(model.predict(X_processed))
        
        predictions = np.array(predictions)
        mean_pred = predictions.mean(axis=0)
        std_pred = predictions.std(axis=0)
        
        return mean_pred, std_pred
    
    @classmethod
    def load(cls, filepath: Path, config: Optional[Config] = None) -> 'SOHPredictor':
        """
        ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        
        Args:
            filepath: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            config: ì„¤ì • ê°ì²´ (Noneì´ë©´ ì €ì¥ëœ ì„¤ì • ì‚¬ìš©)
        
        Returns:
            SOHPredictor ì¸ìŠ¤í„´ìŠ¤
        """
        from .config import Config
        
        config = config or Config()
        trainer = ModelTrainer(config)
        trainer.load(filepath)
        
        return cls(trainer)